{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c2a096-600d-46b1-b9da-568805e0d16e",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5208a-f625-4caa-a36d-c74bb30c4944",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gradient Checking Nedir? \n",
    "### Neden Gradient Checking Kullanıyoruz? \n",
    "### Implementation (Uygulama) Kısmı "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff35144-2e11-4fe4-9244-22b0f8d0d23b",
   "metadata": {},
   "source": [
    "![Lorem Picsum](https://picsum.photos/200/300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c5029-ecf4-4616-9237-f6e16959283d",
   "metadata": {},
   "source": [
    "### Gradient Checking Nedir? \n",
    "İlk olarak Gradient Checking'i neural network yapılarını implementini yani uygulamasını yaptığımız zaman kullanıyoruz.\n",
    "Neural Network implement ederken **forward propagation** ve **back propagation** kullanıyoruz. Kısaca açıklarsak forward propagation ve back propagation'ı, forward propagation\n",
    "bir neural network yapısında input'tan output'a doğru hidden layer arasında giden bir fonksiyon. Elimizdeki weight (ağırlık) değerler ile output'u tahmin etmeye çalışıyor da diyebiliriz. Back propagation ise forward propagation'dan bulduğumuz weight'lerin hatasını minimize etmeye ve en iyi weight değerini bulmaya çalışır. \n",
    "Gradient Checking ise back propagation algoritmamızın hatalarını ayıklar.Kodumuzun doğruluğunu sayısal olarak belirtir  de diyebiliriz. Bir gün patronumuz gelip backpropagion'ın gerçekten çalışıp çalışmadığına dair bir kanıt isterse bunun güvencesini gradient checking algoritması ile verebiliriz. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe9fda-87f2-43ac-90d3-239710ca2946",
   "metadata": {},
   "source": [
    "### Neden Gradient Checking'e İhtiyacımız Var?\n",
    "Back propagation algoritmasında çok fazla ayrıntı vardır ve bazen göze çarpmayan bug'lara hatalara takılabiliriz. Mesela algoritmamızı gradient descent veya başka optimizasyon algoritmalarıyla çalıştırırsak cost fonksiyonumuz her iterasyonda azalıyor gibi durabilir ancak bu durum back propagation uygulaması sırasında bir bug'a takılı kalsa da gerçekleşebilir. İşte burada gradient checking algoritmasını kullanma ihtiyacımız doğar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df7318-e467-487b-9ffb-9af25e5d4566",
   "metadata": {},
   "source": [
    "### Implementation (Uygulama) Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ba9504-a3e5-4a4b-82ae-e44b2eec1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    J = np.dot(theta, x)    \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af1eaad-adff-44f0-8508-ccd6ab3a13c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2094551-a799-4401-88ce-c38a6cc5371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the derivative of J with respect to theta\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- the gradient of the cost with respect to theta\n",
    "    \"\"\"\n",
    "    dtheta = x\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7714e1e3-0f97-4c60-88bf-c93609777f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff2b7b5-7aec-4495-b48c-552af60af83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # \"gradapprox\" formülü\n",
    "    thetaplus = theta + epsilon                               \n",
    "    thetaminus = theta - epsilon                              \n",
    "    J_plus = forward_propagation(x, thetaplus)                \n",
    "    J_minus = forward_propagation(x, thetaminus)              \n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)           \n",
    "\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)                      \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)   \n",
    "    difference = numerator / denominator                               \n",
    "    \n",
    "    # 1e-7 = Great\n",
    "    # 1e-5 = Normal\n",
    "    # 1e-3 = Worry\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print(\"The gradient is correct!\")\n",
    "    else:\n",
    "        print(\"The gradient is wrong!\")\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b16d4b9-7e0e-4275-bb35-8624caa5bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c8638-a462-4d78-9dfb-504896e67b6c",
   "metadata": {},
   "source": [
    "***\"gradapprox\" formülü*** \n",
    "![formula1](https://miro.medium.com/max/622/0*zCHCJ4EOHMgWmIbJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dac58-dfe6-4dbd-b880-26cd1425a221",
   "metadata": {},
   "source": [
    "![formula1](https://miro.medium.com/max/1400/1*poPIZIlVLupYCP2VEiHRpg.png)\n",
    "***\"gradapprox\" ve \"grad\" arasındaki fark formülü***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802c6d9-f0fe-458c-84da-6c4a7cfa5980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
